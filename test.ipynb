{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 18, 4])\n",
      "Loss: 24.95261001586914\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLabelClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_descriptors=18, options_per_descriptor=4):\n",
    "        super(MultiLabelClassificationHead, self).__init__()\n",
    "        self.num_descriptors = num_descriptors\n",
    "        self.options_per_descriptor = options_per_descriptor\n",
    "        \n",
    "        # Create separate linear layers for each descriptor\n",
    "        self.descriptor_layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, options_per_descriptor) for _ in range(num_descriptors)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        outputs = []\n",
    "        for layer in self.descriptor_layers:\n",
    "            descriptor_output = layer(x)  # shape: (batch_size, options_per_descriptor)\n",
    "            descriptor_output = F.softmax(descriptor_output, dim=1)  # Apply softmax for each descriptor\n",
    "            outputs.append(descriptor_output)\n",
    "        \n",
    "        # Stack outputs along a new dimension\n",
    "        return torch.stack(outputs, dim=1)  # shape: (batch_size, num_descriptors, options_per_descriptor)\n",
    "\n",
    "# Example usage\n",
    "input_dim = 512  # This should match the output dimension of your backbone network\n",
    "classification_head = MultiLabelClassificationHead(input_dim)\n",
    "\n",
    "# Assuming 'features' is the output from your backbone network\n",
    "batch_size = 32\n",
    "features = torch.randn(batch_size, input_dim)\n",
    "outputs = classification_head(features)\n",
    "\n",
    "print(f\"Output shape: {outputs.shape}\")  # Should be (batch_size, 18, 4)\n",
    "\n",
    "# Loss function\n",
    "class MultiLabelSoftmaxLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelSoftmaxLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions shape: (batch_size, num_descriptors, options_per_descriptor)\n",
    "        # targets shape: (batch_size, num_descriptors)\n",
    "        loss = 0\n",
    "        for i in range(predictions.size(1)):\n",
    "            loss += self.ce_loss(predictions[:, i, :], targets[:, i])\n",
    "        return loss.mean()\n",
    "\n",
    "# Example of computing loss\n",
    "criterion = MultiLabelSoftmaxLoss()\n",
    "targets = torch.randint(0, 4, (batch_size, 18))  # Random targets for illustration\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"outputs shape: {outputs.shape}\")\n",
    "loss = criterion(outputs, targets)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptmrg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
